<!DOCTYPE html>
<html style="font-size: 16px;" lang="en"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    <meta name="keywords" content="">
    <meta name="description" content="">
    <title>Weekly Logs</title>
    <link rel="stylesheet" href="nicepage.css" media="screen">
<link rel="stylesheet" href="Weekly-Logs.css" media="screen">
    <script class="u-script" type="text/javascript" src="jquery.js" defer=""></script>
    <script class="u-script" type="text/javascript" src="nicepage.js" defer=""></script>
    <meta name="generator" content="Nicepage 6.2.1, nicepage.com">
    <link id="u-theme-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i|Open+Sans:300,300i,400,400i,500,500i,600,600i,700,700i,800,800i">
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <script type="application/ld+json">{
		"@context": "http://schema.org",
		"@type": "Organization",
		"name": "MTE 481 - Capstone"
}</script>
    <meta name="theme-color" content="#478ac9">
    <meta property="og:title" content="Weekly Logs">
    <meta property="og:description" content="">
    <meta property="og:type" content="website">
  <meta data-intl-tel-input-cdn-path="intlTelInput/"></head>
  <body data-path-to-root="./" data-include-products="false" class="u-body u-xl-mode" data-lang="en"><header class="u-clearfix u-header u-header" id="sec-3bb7"><div class="u-clearfix u-sheet u-sheet-1">
        <nav class="u-menu u-menu-one-level u-offcanvas u-menu-1">
          <div class="menu-collapse" style="font-size: 1rem; letter-spacing: 0px;">
            <a class="u-button-style u-custom-left-right-menu-spacing u-custom-padding-bottom u-custom-top-bottom-menu-spacing u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="#">
              <svg class="u-svg-link" viewBox="0 0 24 24"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#menu-hamburger"></use></svg>
              <svg class="u-svg-content" version="1.1" id="menu-hamburger" viewBox="0 0 16 16" x="0px" y="0px" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><g><rect y="1" width="16" height="2"></rect><rect y="7" width="16" height="2"></rect><rect y="13" width="16" height="2"></rect>
</g></svg>
            </a>
          </div>
          <div class="u-custom-menu u-nav-container">
            <ul class="u-nav u-unstyled u-nav-1"><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="Home.html" style="padding: 10px 20px;">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="Weekly-Logs.html" style="padding: 10px 20px;">Weekly Logs</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="About.html" style="padding: 10px 20px;">About</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="Contact.html" style="padding: 10px 20px;">Contact</a>
</li></ul>
          </div>
          <div class="u-custom-menu u-nav-container-collapse">
            <div class="u-black u-container-style u-inner-container-layout u-opacity u-opacity-95 u-sidenav">
              <div class="u-inner-container-layout u-sidenav-overflow">
                <div class="u-menu-close"></div>
                <ul class="u-align-center u-nav u-popupmenu-items u-unstyled u-nav-2"><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Home.html">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Weekly-Logs.html">Weekly Logs</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="About.html">About</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Contact.html">Contact</a>
</li></ul>
              </div>
            </div>
            <div class="u-black u-menu-overlay u-opacity u-opacity-70"></div>
          </div>
        </nav>
        <img class="u-image u-image-contain u-image-default u-image-1" src="images/university-of-waterloo-1.svg" alt="" data-image-width="2500" data-image-height="1724">
      </div></header>
    <section class="u-clearfix u-section-1" id="sec-50cb">
      <div class="u-clearfix u-sheet u-sheet-1">
        <h1 class="u-text u-text-default u-text-1">Group Weekly Logs</h1>
      </div>
    </section>
    <section class="u-clearfix u-section-2" id="sec-3273">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <h2 class="u-text u-text-default u-text-1">MTE 482 - Development Log</h2>
      </div>
    </section>
    <section class="u-clearfix u-section-3" id="sec-3095">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-text u-text-default u-text-1">
          <span style="font-weight: 700;">Servo: </span>
          <br>- Wired up a servo to the board and testing that it worked by sending it PWM signals. <br>- Developed a routine for characterizing servos by alternating between the full angle range PWM signals. <br>- Created a sweep algorithm that moves servos in increments of 30° from 30° to 150°, for a total of 25 positions across the pitch and yaw servos. <br>- Up until now, visual inspection was used to approximate angles, but now a relatively precise test rig was created to measure the exact angles reached by the servos. <br>- Wrote code for servo communication between the mini PC and RP Pico. Servos can be moved by the mini PC by sending mutation messages to the RP Pico, which subsequently calculates and send the appropriate PWM signal to the servo. Servos were required to reach each position within 200ms of mini PC request initiation. <br>- The test rig was used to characterize both main servos to be used on the prototype, but were found to have specific unacceptable behaviours: <br>1. When given the reference PWM signals given in the specification, both servos failed to reach their required ranges: expected 0° to 120°, but reached roughly 136°. This may be partially caused by the fact that the specification expects 6V power, but we give it 7V. <br>2. "Warm up" seems to be required, where when alternating from 0° to 120°, the 0° end slowly drifts up to ~6° over the course of a couple of minutes and the 120° end drifts up to ~123­°. After a few minutes though, the drift stops, and both ends are reached within around ±1° to ±2°. <br>- Calculations give that at full distance (6m), to reach the required precision of 5cm for RBGD camera measurements, servo angles must be within around ±0.5° to ±1° of the target. Given the issues with drift, "warm up", and full range angle discrepancy, the servos that are currently in use are unlikely to work, so more precise servos are being considered. <br>- A more precise servo would put the system over the attachment weight limit for Skyport V1, so we are considering using only one servo for pitch, removing yaw control. <br>- A couple of more precise servos have been found, with one servo apparently reaching ±1° positions, but it is unlikely that this will be used, as this servo is quite expensive and would require a different mount. So, for the time being a static mount was printed with no servo connections, relying solely on drone motion. In the future, a singular servo may be used with hard stops, and a current sensor to stop powering the servo when it stalls (when the hard stop is reached). <br>
          <br>
          <span style="font-weight: 700;">Reconstruction: </span>
          <br>
          <br>- Researched the Open3D library, specifically on its pipeline and integration components, for performing point cloud reconstruction from multiple RGBD images correlated with global pose data. <br>- Performed some initial testing using scalable TSDF volumes using provided example datasets, such as SampleRedwoodRGBDImages (5 images), SampleFountainRGBDImages (33 images), LoungeRGBDImages (3000 images), and BedroomRGBDImages (~22000 images). See https://www.open3d.org/docs/release/index.html and https://www.open3d.org/docs/release/python_api/open3d.data.html for reference. In these tests, the base pipeline was brought up using  single TSDF volume object, which integrates all RBGD images with odometry data in one go, before converting to a point cloud and exporting. One issue of note that was found was memory usage. Smaller datasets were more than manageable with a single TSDF volume, but larger volumes resulted in a lot of RAM usage. Specifically, LoungeRGBDImages with its 3000 images peaked at around 12 to 14 GB of RAM used when all images were loaded into the TSDF volume. The BedroomRGBDImages dataset (with ~22000 images) slowly climbed until it reached ~80% RAM usage (~20 GB), before the system slowed to a halt, like due to the OS swapping memory to and from disk. This eventually caused the system to freeze and crash. For a large dataset with many images, a different method than using a single TSDF volume is required. <br>- A few methods were suggested for reducing RAM usage, and the chosen method is as follows. Segment the point cloud into blocks of 6m cubes (which corresponds to the maximum depth distance possible). These blocks are then subdivided into chunks of 1m or 2m. Only the current *block* that camera is located within is operated on, which is stored as a TSDF volume in RAM. Now, when the camera moves out of its current *chunk*, the block corresponding to its new position is computed. Any chunks that overlap between the current and new block remain in memory, while the others are loaded/unloaded as necessary. When unloaded, the current TSDF volume block is converted to a point cloud, and this point cloud is partitioned by chunks. Chunks that need to be unloaded are written as a point cloud to a file whose name is the coordinates of a corner of the chunk. When loading a chunk, the point cloud file is read from disk and re-integrated into the TSDF volume block. This reduces the RAM footprint reconstruction as only the immediate neighbourhood is stored in memory at any time. <br>- We found that this method could not be implemented, due to the fact that exporting a TSDF to a point cloud removes weight information for each image, and so when reading a point cloud file, it cannot be re-integrated into the TSDF volume. So, other methods were considered, such as using a separate TSDF volume for each chunk in a block, and writing/reading the TSDF object itself to/from a file (instead of converting to a point cloud), but it was found that this results in a lot of duplicate data, and that partitioning into chunks can only be performed on a point cloud, and not a TSDF volume. <br>- The final method we settled on was simply storing a TSDF volume with a rollover limit on how many images it can take. Once the limit has been reached, the TSDF volume is converted to a point cloud, is partitioned on each chunk, and the chunks are dumped to files. Then, whenever a chunk is to be dumped to file when a file already exists for that chunk, the file is loaded, and manual reconstruction is performed with ICP (iterative closest point) registration (see https://www.open3d.org/html/tutorial/pipelines/icp_registration.html). This reconstruction merges two point clouds, and the resulting point cloud is dumped back to the file. It is important to note however, that due to the lack of weighting data in point clouds, the reconstruction is somewhat less accurate than simply combining all images in a single TSDF volume. <br>- Parameters were then varied, such as the rollover limit, such that the dumped data could be rendered at roughly 5 fps. <br>
          <br>
          <br>
          <span style="font-weight: 700;">Mechanical: </span>
          <br>
          <br>- Designed and engineered a modular housing box to securely contain onboard components, integrating attachable antenna mounts, a removable lid for accessibility, and precise Skyport mounting points for seamless drone integration. <br>- Conducted weight optimization to achieve a final net mass of 1.34 kg while maintaining structural integrity. <br>- 3D printed the housing using PLA, balancing print quality, strength, and weight. Employed M2/M3 screws for assembly, ensuring component stability while allowing for ease of disassembly and modification. <br>
          <br>
          <span style="font-weight: 700;">Interactive 3D Visualization Interface: </span>
          <br>
          <br>- Developed a web-based platform capable of dynamically loading and rendering PCD mesh files, allowing users to interact with and explore 3D models in semi-real time. Integrated VR support, enabling immersive visualization and navigation of the mesh environment. <br>- Uses HTTPS server and Flask backend to serve files in local directory <br>
          <br>
        </p>
      </div>
    </section>
    <section class="u-clearfix u-section-4" id="carousel_015c">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <h2 class="u-text u-text-default u-text-1">MTE 481 - Development Log</h2>
      </div>
    </section>
    <section class="u-clearfix u-section-5" id="sec-8e5d">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-text u-text-default u-text-1">The following section details the weekly logs from each member of the team.</p>
      </div>
    </section>
    <section class="u-clearfix u-section-6" id="sec-93d8">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-text u-text-1">
          <span style="font-weight: 700;"> 09/09/24</span>
          <br>Currently in ideation and
research stage for selecting ideas. Team has been formed as follows:&nbsp;<br>Sean
Clarke, Maadhava Ravishankar, Rukshi Thiyagayogan, Henry Zhou.&nbsp;<br>
          <br>The initial
ideas proposed:&nbsp;<br>- Fire search and rescue
robot&nbsp;<br>- Security monitoring drone
swarm<br>-&nbsp;Anti-car theft system (to
prevent CAN injections)<br>- Surveying drone system&nbsp;&nbsp;<br>
          <br>
          <span style="font-weight: 700;">17/09/24</span>
          <br>After consultation with
advisors, further research and contacting agencies the following conclusions
were reached about each of the ideas:&nbsp;<br>
          <br>
          <span style="text-decoration: underline !important;">Fire search and rescue
robot</span>
          <br>- Spoke with Troy FD in
Michigan about use cases for robotics in firefighting applications, and was
told that the most applicable cases would likely be in urban environments. Was
redirected to contact Toronto FD and Mississauga FD instead.&nbsp; &nbsp;<br>
          <br>
          <span style="text-decoration: underline !important;">Security monitoring drone
swarm</span>
          <br>- Drones are very expensive,
obtaining/working with multiple of them is not feasible given our time frame
and budget&nbsp;&nbsp;<br>
          <br>
          <span style="text-decoration: underline !important;">Anti-car theft system
(to prevent CAN injections)</span>
          <br>- Manufacturer dependent, no
access to a physical system to test with&nbsp;<br>- Schematics are proprietary,
no information available online&nbsp;<br>- Problem is already set to
be solved in software primarily; no mechanical aspects&nbsp;&nbsp;<br>
          <br>
          <span style="text-decoration: underline !important;">Surveying Drone System</span>
          <br><!--[if !supportLists]-->-&nbsp;<!--[endif]-->Not enough budget to
develop custom drone, however RoboHub has drones available we can develop a
payload for&nbsp;<br>- Met with Branon DeHart to
discuss options&nbsp;<br>- Potentially integrate LiDAR
and RTK GPS to obtain precision accuracy&nbsp;&nbsp;<br>
          <br>
          <span style="font-weight: 700;">24/09/24</span>
          <br>
          <br>
          <span style="text-decoration: underline !important;">Fire robot</span>
          <span style="text-decoration: underline !important;">:</span>&nbsp;&nbsp;<br><!--[if !supportLists]-->-&nbsp;<!--[endif]-->Talked to multiple Toronto
Fire agency representatives including the fire safety and engineering team.
Spoke with other fire departments, including Troy FD in Michigan, and attempted
to visit a fire station in Mississauga. They indicated the best use case for
robotics would be for search and rescue applications in condos, high-rises, and
other densely packed spaces.&nbsp;<!--[if !supportLists]-->
          <br>
          <br>- Would require very
expensive materials to build a robot capable of withstanding fire temperatures,
as well as electronics that are capable of withstanding the operating
conditions.&nbsp;<br>
          <br>- Would be very difficult to
navigate terrain, drone system would have limited flight time (current record
by a company for fireproof drone is 10 minutes)<br>
          <br>
          <span style="text-decoration: underline !important;">Architectural/Construction
Surveying:</span>
          <br>- Continued research on
problem, two approaches on which aspects of surveying to address: <!--[if !supportLists]-->§&nbsp;&nbsp;<!--[endif]-->
          <br>
          <br>Indoor - Would have to be able to
network outside house for RTK GPS to work, must avoid obstacles/ be
nimble enough<br>Outdoor - Be able to withstand
external weather conditions, must be able to fly
high/scan a large volume&nbsp;&nbsp;<br>
          <br>
          <br>
          <span style="font-weight: 700;">01/10/24</span>
          <br>Got in touch with Brandon
DeHart from RoboHub about our project, design, and parts selection/sourcing&nbsp;<br><!--[endif]-->
          <br>4 alternative approaches
were developed:&nbsp;<br>
          <br><b>
            <span style="font-weight: 400;">Solution 1 - Drone/Stereo Camera</span></b>
          <br><!--[endif]-->Quadcopter based drone<br><!--[endif]-->Navigates via GPS<br><!--[endif]-->Image computation is
preformed on the back-end <br>
          <br><!--[endif]--><b>
            <span style="font-weight: 400; text-decoration: underline !important;">Pros:</span></b>
          <br><!--[endif]-->Stereo camera is high
resolution<br><!--[endif]-->Aerial provides greater 3D
mapping<br><!--[endif]-->Can navigate obstacles
easier<br><!--[endif]-->Easier to deploy than
ground based solution&nbsp;<br>
          <br><b>
            <span style="font-weight: 400; text-decoration: underline !important;">Cons:</span></b>
          <br><!--[endif]-->Expensive<br><!--[endif]-->Hard to develop (software
&amp; mechanical) <br><!--[endif]-->Limited battery life&nbsp;<br>
          <br><b>
            <span style="text-decoration: underline !important;">Solution 2 - Drone/Lidar</span></b>
          <span style="font-weight: 400;">
            <br><!--[endif]-->Quadcopter based drone<br><!--[endif]-->Navigates via GPS<br><!--[endif]-->Image computation is
preformed on the back-end&nbsp;
          </span>
          <br>
          <br>
          <span style="font-weight: 400;"><b>
              <span style="font-weight: 400; text-decoration: underline !important;">Pros:</span></b>
            <br><!--[endif]-->Cheaper than Stereo Camera<br><!--[endif]-->Simpler software for image
processing<br><!--[endif]-->Aerial provides greater 3D
mapping<br>
          </span><!--[endif]-->Can navigate obstacles
easier<br><!--[endif]-->Easier to deploy than
ground based solution<br>
          <br><b>
            <span style="font-weight: 400; text-decoration: underline !important;">Cons:</span></b>
          <br><!--[endif]-->Not as accurate<br><!--[endif]-->Requires greater drone
stability than stereo camera<br><!--[endif]-->Expensive<br><!--[endif]-->Hard to develop (software
&amp; mechanical)<br><!--[endif]-->Limited battery life&nbsp;<br>
          <br><b>
            <span style="font-weight: 400; text-decoration: underline !important;">Solution 3 - Ground Based Car</span></b>
          <br><!--[endif]-->Quad wheel/tank<br><!--[endif]-->On board IMU<br><!--[endif]-->GPS Signal<br><!--[endif]-->Stereo Camera&nbsp;<br>
          <br><b>
            <span style="font-weight: 400; text-decoration: underline !important;">Pros:</span></b>
          <br><!--[endif]-->Easier than aerial for
development of mechanics<br><!--[endif]-->Easier to stabilize than
aerial for controls<br><!--[endif]-->Easier image processing<br>
          <br><b>
            <span style="font-weight: 400; text-decoration: underline !important;">Cons:</span></b>
          <br><!--[endif]-->Hard to navigate certain
terrains<br><!--[endif]-->Need to adjust position to
get full resolution of mapping&nbsp;<br>
          <br><b>
            <span style="font-weight: 400; text-decoration: underline !important;">Solution 4 - Individual Base Station</span></b>
          <br><!--[endif]-->Set up around the room that
locate each other to take measurements <!--[if !supportLists]-->-<br><!--[endif]-->GPS signals&nbsp;<br>
          <br><b>
            <span style="font-weight: 400; text-decoration: underline !important;">Pros:</span></b>
          <br><!--[endif]-->Easy to develop<br><!--[endif]-->Longer battery time<br><!--[endif]-->Doesn’t move, lower
complexity<br><!--[endif]-->Potentially higher
precision<br>
          <br><b>
            <span style="font-weight: 400; text-decoration: underline !important;">Cons:</span></b>
          <br><!--[endif]-->Costly<br><!--[endif]-->Have to create a lot<br><!--[endif]-->Have to manually set up<br><!--[endif]-->Add label<br>
          <br><!--[endif]-->
          <span style="text-decoration: underline !important;">Patents Researched: </span>
          <br> SYSTEM AND METHOD FOR STRUCTURAL INSPECTION
AND CONSTRUCTION ESTIMATION USING AN UNMANNED AERIAL VEHICLE <!--[if !supportLists]-->-<br><!--[endif]-->
          <a href="https://www.ic.gc.ca/opic-cipo/cpd/eng/patent/3012049/summary.html?query=architectural+survey+drone&amp;type=basic_search" class="u-active-none u-border-none u-btn u-button-style u-hover-none u-none u-text-palette-1-base u-btn-1">Canadian
Patent Database / Base de données sur les brevets canadiens</a><!--[if !supportLists]-->-<br><!--[endif]-->Status: Deemed Abandoned <!--[if !supportLists]-->-<br><!--[endif]-->An automated image
capturing and processing system and method may allow a field user to operate a
UAV via a mobile computing device to capture images of a structure area of
interest (AOI). The mobile computing device receives user and/or third party
data and creates UAV control data and a flight plan. The mobile computing
device executes a flight plan by issuing commands to the UAV's flight and
camera controller that allows for complete coverage of the structure AOI. After
data acquisition, the mobile computing device then transmits the UAV output
data to a server for further processing. At the server, the UAV output data can
be used for a three- dimensional reconstruction process. The server then
generates a vector model from the images that precisely represents the
dimensions of the structure. The server can then generate a report for
inspection and construction estimation. <br>
          <br>CAPABLE OF
NAVIGATING AND/OR MAPPING ANY MULTI-DIMENSIONAL SPACE-METHOD&nbsp;<br><!--[endif]-->
          <a href="https://www.ic.gc.ca/opic-cipo/cpd/eng/patent/2683726/summary.html?query=architectural+survey+drone&amp;type=basic_search" class="u-active-none u-border-none u-btn u-button-style u-hover-none u-none u-text-palette-1-base u-btn-2">Canadian
Patent Database / Base de données sur les brevets canadiens</a><!--[if !supportLists]-->-<br><!--[endif]-->Status: Deemed Abandoned
and Beyond the Period of Reinstatement - Pending Response to Notice of
Disregarded Communication <!--[if !supportLists]-->-<br><!--[endif]-->A method and system that
allows a user to perform automatic study, layout and verification of a
multidimensional space in real time where the study can be displayed
graphically, in 3 -dimensions for example, via a handheld unit allowing the
system to guide and/or navigate the user throughout the multidimensional space
as the automatic study and/or layout is being performed.
DIGITAL TERRAIN&nbsp;<br>
          <br>MAPPING WITH GPS
AND LASER SYSTEM <!--[if !supportLists]-->-<br><!--[endif]-->
          <a href="https://www.ic.gc.ca/opic-cipo/cpd/eng/patent/2669351/summary.html?query=%28construction+survey%29+AND+%28current-status%3APatents%29&amp;type=advanced_search" title="https://www.ic.gc.ca/opic-cipo/cpd/eng/patent/2669351/summary.html?query=%28construction+survey%29+AND+%28current-status%3APatents%29&amp;type=advanced_search" class="u-active-none u-border-none u-btn u-button-style u-hover-none u-none u-text-palette-1-base u-btn-3">https://www.ic.gc.ca/opic-cipo/cpd/eng/patent/2669351/summary.html</a><!--[if !supportLists]-->-<br><!--[endif]-->Status: Granted and Issued <!--[if !supportLists]-->-<br><!--[endif]-->This invention concerns
digital terrain mapping, and in particular a method for producing digital
terrain maps of the vicinity around large rotating machinery, such as
draglines, shovels, excavators, wheel loaders or cranes. The method involves
the following steps: Mounting a 2-Dimensions laser scanner and a high-accuracy
RTK GPS system to an item of large rotating machinery, at a radial extremity of
the machinery. Arranging the laser scanner to record radial lines of data
representing the terrain below the extremity of the machinery. Rotating the
machinery completely about its axis of rotation and generating data from both
the laser scanner and GPS system that represents the terrain around the
machinery. Wherein, a calibration is conducted by recording data from markers
at known locations in the terrain below the extremity of the machinery to
determine the relative positions of the laser scanner centre and the GPS
antenna, the orientation of the laser scanner and the time lag between the data
generated from both the laser scanner and the GPS system. In a further aspect
the invention concerns equipment for use in the method and software.&nbsp;<br>
          <br>
          <span style="font-weight: 700;">9/10/24</span>
          <br>Changed the 4th
proposed solution of individual base stations with an omni-wheel wheel robot
that has a high-resolution camera to perform Structure from Motion (SfM)
photogrammetry, as the base station design did not meet sufficient constraints.&nbsp;<br>
          <br>Met Brandon in person and
he redirected us to contact several companies including VSemi and the VIP lab
about sourcing/borrowing parts.<br>
          <br>At Brandon’s suggestion,
the following change was made:<br>- A combination of solution 1
and 2 was selected – Stereo Camera with LiDAR integration. In addition, our
scope was narrowed to exclusively working outside of a building, with a focus
on residential housing as a first step.&nbsp;&nbsp;<br>
          <br>
          <br>
          <span style="font-weight: 700;">15/10/24</span>
          <br>Work has commenced on PDP
presentation. Mechanical, Electrical, and Software tasks have been divided as
such:&nbsp;<br>-&nbsp;&nbsp; <!--[endif]-->Mechanical Enclosure –
Rukshi&nbsp;<br>-&nbsp;&nbsp; <!--[endif]-->Mechanical Camera Gimbal –
Sean&nbsp;<br>-&nbsp;&nbsp; <!--[endif]-->Electrical – Rukshi <!--[if !supportLists]-->o&nbsp;&nbsp; <!--[endif]-->Proposed solutions -
Maadhava&nbsp;<br>-&nbsp; &nbsp;<!--[endif]-->Hardware Integration –
Maadhava&nbsp;<br>-&nbsp;&nbsp; <!--[endif]-->Software – Henry&nbsp;&nbsp;<br>
          <br>
          <span style="font-weight: 700;">22/10/24</span>
          <br>PDP presentation has been
finished and covers the use case, alternate solutions, timeline and rankings
for each solution.<br>
          <br>Failed to source a LiDAR
that can be mounted on a drone and works outdoors (VSemi did not respond yet)<br>
          <br>Spoke to research students
at the CViSS Lab regarding what gaps exist in drone 3D mapping&nbsp;<br>
          <br>- Drone mapping is already a
well understood problem, with many platforms supporting photogrammetry and SfM&nbsp;<br>- Processing happens offline
and requires heavy cloud computation&nbsp;<br>
          <br>Pivoted to heavily
emphasize on real time 3D reconstruction, as this area has not been explored
and enables obstacle detection and single-pass reconstruction without the need
to fail in missed areas&nbsp;<br>
          <br>Design 1 (drone with RGBD)
was chosen instead of design 2 (drone with RGBD + LiDAR) because it better
supports real time processing and because of the inability to source a LiDAR
that meets requirements<br>
          <br>A BOM has been developed,
and the following components have also been purchased due to large lead times:&nbsp;<br>
          <br>- Intel D455f RealSense Depth
Camera (82635DSD455F)<br>
          <a href="https://fleetnetwork.ca/products/intel-d455f-realsense-depth-camera-82635dsd455f.html?sku=82635DSD455F&amp;utm_source=googleshopping&amp;utm_medium=cse&amp;gad_source=1" class="u-active-none u-border-none u-btn u-button-style u-hover-none u-none u-text-palette-1-base u-btn-4">https://fleetnetwork.ca/products/intel-d455f-realsense-depth-camera-82635dsd455f.html?sku=82635DSD455F&amp;utm_source=googleshopping&amp;utm_medium=cse&amp;gad_source=1</a><u></u>&nbsp;&nbsp;<br>- UM982 high-precision
Dual-RTK GNSS Receiver SoC-NebulasIV multi-frequency dual-antenna with Helix
Antenna Type C to USB board<a href="https://www.aliexpress.us/item/1005007287184287.html?spm=a2g0o.detail.pcDetailTopMoreOtherSeller.1.4358YqbfYqbfJm&amp;gps-id=pcDetailTopMoreOtherSeller&amp;scm=1007.40050.354490.0&amp;scm_id=1007.40050.354490.0&amp;scm-url=1007.40050.354490.0&amp;pvid=bc8cb7d3-7efb-4094-a175-ddbbdff8c7c7&amp;_t=gps-id%3ApcDetailTopMoreOtherSeller%2Cscm-url%3A1007.40050.354490.0%2Cpvid%3Abc8cb7d3-7efb-4094-a175-ddbbdff8c7c7%2Ctpp_buckets%3A668%232846%238116%232002&amp;pdp_npi=4%40dis%21CAD%21260.48%21208.38%21%21%211308.29%211046.61%21%402103209b17301346265973510e4dc8%2112000040074568207%21rec%21CA%21%21ABX&amp;utparam-url=scene%3ApcDetailTopMoreOtherSeller%7Cquery_from%3A&amp;gatewayAdapt=glo2usa" class="u-active-none u-border-none u-btn u-button-style u-hover-none u-none u-text-palette-1-base u-btn-5">https://www.aliexpress.us/item/1005007287184287.html</a>
          <br>
          <br>
          <span style="font-weight: 700;">29/10/24</span>
          <br>Detailed design has
commenced. We are considering borrowing a Matrice drone from RoboHub. It was
recommended that we use the Matrice 210 V1 from RoboHub. The system will
consist of a gimbal mounted underneath a box containing the payload
electronics.&nbsp;<!--[if !supportLists]-->
          <br>
          <br>Prototype interface code
(driver) for the UM982 driver started, including research into NTRIP and the
NMEA 0183 protocol it uses to send GPS data. Some open source libraries were
found that support our use case.&nbsp;<br>
          <br>CAD designs for the payload
with selected components are develope<br>
        </p>
      </div>
    </section>
    <section class="u-clearfix u-section-7" id="sec-2507">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <img class="u-image u-image-default u-image-1" src="images/image5.png" alt="" data-image-width="533" data-image-height="236">
      </div>
    </section>
    <section class="u-clearfix u-section-8" id="sec-c129">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-text u-text-default u-text-1">Internal component
placement was selected to fit within a 200mm by 200mm build plate&nbsp;(for 3D printing)&nbsp;</p>
      </div>
    </section>
    <section class="u-clearfix u-section-9" id="sec-407b">
      <div class="u-clearfix u-sheet u-valign-bottom u-sheet-1">
        <img class="u-image u-image-contain u-image-default u-image-1" src="images/image6.png" alt="" data-image-width="600" data-image-height="311">
      </div>
    </section>
    <section class="u-clearfix u-section-10" id="sec-e54c">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-text u-text-default u-text-1"> Hand calculations were performed
for gimbal design to validate operation in moderate conditions.
CAD designs for the gimbal
have been developed. Wires were routed internally, and we were able to reuse
(2x) MG966R servo motors that Sean previously had. Design will be 3D printed
and assembled for early stage prototyping.&nbsp;&nbsp;</p>
      </div>
    </section>
    <section class="u-clearfix u-section-11" id="sec-c970">
      <div class="u-clearfix u-sheet u-sheet-1">
        <img class="u-image u-image-contain u-image-default u-preserve-proportions u-image-1" src="images/image3.png" alt="" data-image-width="244" data-image-height="225">
      </div>
    </section>
    <section class="u-clearfix u-section-12" id="carousel_bd77">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-text u-text-1">
          <span style="font-weight: 700;"></span>
          <span style="font-weight: 700;">5/11/24</span>
          <br>Camera gimbal prototype has
been assembled and functions well. There were some 3D printing issues that
needed to be mitigated (i.e. adjusting wall thicknesses) but the rest printed
well as shown. The design will need to be reprinted for the final product as
the height of the mounting arm needs to be reduced otherwise it will hit the
ground.<br>
          <br>
          <span style="font-weight: 700;"></span>
        </p>
      </div>
    </section>
    <section class="u-clearfix u-section-13" id="sec-7d61">
      <div class="u-clearfix u-sheet u-sheet-1">
        <img class="u-expanded-height u-image u-image-contain u-image-default u-image-1" src="images/image4.png" alt="" data-image-width="225" data-image-height="310">
      </div>
    </section>
    <section class="u-clearfix u-section-14" id="sec-f47a">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-text u-text-default u-text-1">Started work on the WebRTC
protocol stack required to transport video data off the drone for processing.
WebRTC is a complex collection of protocols and systems for establishing a UDP
connection between two devices (peers), neither of which has a public IP. Some
work will be required to fully understand the UDP hole punching mechanism.&nbsp;<br>
          <br>Got in touch with Robert
Wagner from RoboHub about taking a look at the V1 drone in person.&nbsp;&nbsp;<br>
          <br>
          <span style="font-weight: 700;">12/11/24</span>
          <br>Progress has been made on
testing the servo actuation using an Arduino and basic motor driver, range of
motion is good. Wires do not get caught. Helped Henry with software validation
testing.&nbsp;<br>
          <br>Got in touch with Alexander
Werner from RoboHub and visited him in person to measure the V1 drone and take
important dimensions for the SkyPort mounting.&nbsp;<br>
          <br>Finalized borrowing of the
V1 drone, with each group member completing required training.<br>
          <br>WebRTC requires a
standalone channel for initial signaling before the connection can be
established. A HTTP server prototype was designed for this.<br>
          <br>RealSense camera received
and tested. Success in extracting the depth stream and muxing it with the RGB
stream for a unified camera stream, using the recommended “Hue” color map to
encode depth information.&nbsp;<br>
          <br>Began looking into
correlation mechanisms between the image data and GPS pose data. Two initial
ideas are being researched:
o&nbsp; -&nbsp;Encoding the GPS data into
each video frame for decoding by the host device.&nbsp;<br>- Sending the GPS data
through a channel separate from the video stream, and stamping each video and
GPS frame with an ID for correlation.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<br>
          <br>PCB Schematic review was
performed to ensure functionality&nbsp; &nbsp;<br>
          <br>
          <span style="font-weight: 700;">19/11/24</span>
          <br>Development of FDP
presentation. Mounted gimbal to a plank of wood for ease of testing in data
calibration<br>
          <br>Success in establishing a
WebRTC connection between two hosts over UDP and streaming video. Refactor of
the WebRTC protocol modules, signaling modules, and ffmpeg encode/decode into a
standalone library underway.&nbsp;&nbsp;<br>
          <br>
          <span style="font-weight: 700;"> 26/11/24</span>
          <br>RTK GPS driver completed
and tested.<br>
          <br>Decoding/encoding of GPS
binary data into macroblocks was chosen as the preferred correlation mechanism.
It was then implemented and tested.&nbsp;<br>
          <br>Initial prototype of 3D
reconstruction using TSDF (truncated signed depth function) integration
implemented and tested on 5 depth&nbsp;<br>
          <br>images. This yielded an accurate mesh.<br>
          <br>Presentation finalized and
delivered, went well.&nbsp;Work has begun on the final
report. Section assignment as follows:
&nbsp;<br>- Letter of transmittal -
Maadhava&nbsp;<br>- Executive summary - Sean&nbsp;<br>- Introduction – Maadhava,
Henry&nbsp;<br>- Proposed Solutions and
Comparison – Maadhava, Henry, Rukshi&nbsp;<br>- Design – Henry, Rukshi,
Sean&nbsp;<br>- Schedule and budget – Sean&nbsp;<br>- Conclusions and
Recommendations – Maadhava, Henry&nbsp;&nbsp;<br>
          <br>
          <span style="font-weight: 700;">2</span>
          <span style="font-weight: 700;">/12/24</span>
          <br>Worked on report throughout
the week. Completed and submitted the final report.&nbsp;&nbsp;
        </p>
      </div>
    </section>
    
    
    
    <footer class="u-align-center u-clearfix u-footer u-grey-80 u-footer" id="sec-c732"><div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-small-text u-text u-text-variant u-text-1">SkyMap 2024 - University of Waterloo</p>
      </div></footer>
    <section class="u-backlink u-clearfix u-grey-80">
      <a class="u-link" href="https://nicepage.com/website-templates" target="_blank">
        <span>Website Templates</span>
      </a>
      <p class="u-text">
        <span>created with</span>
      </p>
      <a class="u-link" href="https://nicepage.best" target="_blank">
        <span>Website Builder</span>
      </a>. 
    </section>
  
</body></html>